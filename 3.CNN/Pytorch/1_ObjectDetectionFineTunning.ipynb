{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this tutorial, we will be finetuning a pre-trained Mask R-CNN model on the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an object detection and instance segmentation model on a custom dataset.\n",
        "\n",
        "## Defining the Dataset\n",
        "The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard torch.utils.data.Dataset class, and implement __len__ and __getitem__.\n",
        "\n",
        "The only specificity that we require is that the dataset __getitem__ should return a tuple:\n",
        "\n",
        "1. mage: torchvision.tv_tensors.Image of shape [3, H, W], a pure tensor, or a PIL Image of size (H, W)\n",
        "\n",
        "2. target: a dict containing the following fields\n",
        "\n",
        "  1. boxes, torchvision.tv_tensors.BoundingBoxes of shape [N, 4]:\n",
        "the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H\n",
        "\n",
        "  2. labels, integer torch.Tensor of shape [N]: the label for each bounding box.\n",
        "0 represents always the background class.\n",
        "\n",
        "  3. image_id, int: an image identifier. It should be\n",
        "unique between all the images in the dataset, and is used during evaluation\n",
        "\n",
        "  4. area, float torch.Tensor of shape [N]: the area of the bounding box. This is used\n",
        "during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "\n",
        "  5. iscrowd, uint8 torch.Tensor of shape [N]: instances with iscrowd=True will be\n",
        "ignored during evaluation.\n",
        "\n",
        "  6. (optionally) masks, torchvision.tv_tensors.Mask of shape [N, H, W]: the segmentation\n",
        "masks for each one of the objects"
      ],
      "metadata": {
        "id": "uFpNIpbeiNAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has both classes, your labels tensor should look like [1, 2].\n",
        "\n",
        "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratios), then it is recommended to also implement a get_height_and_width method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via __getitem__ , which loads the image in memory and is slower than if a custom method is provided.\n",
        "\n",
        "## Writing a custom dataset for PennFudan\n",
        "Let’s write a dataset for the PennFudan dataset. After downloading and extracting the zip file, we have the following folder structure:"
      ],
      "metadata": {
        "id": "sTmzZ-jxim3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PennFudanPed/\n",
        "    \n",
        "    PedMasks/\n",
        "        \n",
        "        FudanPed00001_mask.png\n",
        "        FudanPed00002_mask.png\n",
        "        FudanPed00003_mask.png\n",
        "        FudanPed00004_mask.png\n",
        "        ...\n",
        "    PNGImages/\n",
        "        FudanPed00001.png\n",
        "        FudanPed00002.png\n",
        "        FudanPed00003.png\n",
        "        FudanPed00004.png"
      ],
      "metadata": {
        "id": "EgGXQE0Wk12E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRjz7BpziceU",
        "outputId": "9364ce1d-9731-4483-af6b-8e924766215f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/gautamchitnis/cocoapi.git (to revision cocodataset-master) to /tmp/pip-req-build-qr8ls7li\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/gautamchitnis/cocoapi.git /tmp/pip-req-build-qr8ls7li\n",
            "  Running command git checkout -b cocodataset-master --track origin/cocodataset-master\n",
            "  Switched to a new branch 'cocodataset-master'\n",
            "  Branch 'cocodataset-master' set up to track remote branch 'cocodataset-master' from 'origin'.\n",
            "  Resolved https://github.com/gautamchitnis/cocoapi.git to commit 20291f19c46a8d11935862bc9e449a1b72ec25ed\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a torch.utils.data.Dataset class for this dataset. In the code below, we are wrapping images, bounding boxes and masks into torchvision.TVTensor classes so that we will be able to apply torchvision built-in transformations (new Transforms API) for the given object detection and segmentation task.\n",
        "\n",
        "Namely, image tensors will be wrapped by torchvision.tv_tensors.Image, bounding boxes into torchvision.tv_tensors.BoundingBoxes and masks into torchvision.tv_tensors.Mask. As torchvision.TVTensor are torch.Tensor subclasses, wrapped objects are also tensors and inherit the plain torch.Tensor API. For more information about torchvision tv_tensors see this documentation."
      ],
      "metadata": {
        "id": "GU-Oa3hGjFI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "yNIlscwliqYG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining your model\n",
        "In this tutorial, we will be using Mask R-CNN, which is based on top of Faster R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image."
      ],
      "metadata": {
        "id": "QIlME7XNm9y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1 - Finetuning from a pretrained model\n",
        "Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:"
      ],
      "metadata": {
        "id": "oOR_TyqYph5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "#load pre-trained model\n",
        "model =  torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
      ],
      "metadata": {
        "id": "-rhAN1Lkm0wj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2 - Modifying the model to add a different backbone"
      ],
      "metadata": {
        "id": "KOAoTRPjzGCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# ``FasterRCNN`` needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "    featmap_names=['0'],\n",
        "    output_size=7,\n",
        "    sampling_ratio=2,\n",
        ")\n",
        "\n",
        "# put the pieces together inside a Faster-RCNN model\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=2,\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler,\n",
        ")\n"
      ],
      "metadata": {
        "id": "G-Afk2lgzAug"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object detection and instance segmentation model for PennFudan Dataset\n",
        "In our case, we want to finetune from a pre-trained model, given that our dataset is very small, so we will be following approach number 1.\n",
        "\n",
        "Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:"
      ],
      "metadata": {
        "id": "FcsXFzNyzbPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JbxCm3NZzXGI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting everything together\n",
        "In references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py and references/detection/utils.py. Just download everything under references/detection to your folder and use them here. On Linux if you have wget, you can download them using below commands"
      ],
      "metadata": {
        "id": "R7p19Mlrzeli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfRkd-Tizc5M",
        "outputId": "d03dbff7-d06a-4130-94c0-3cec7ed09153"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "Yfqnsu9_zgCa"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}